# -*- coding: utf-8 -*-
"""swin_llama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v7wIHu9ZyLw_HEP6Avt_QXpUqDTZjOtX
"""

# prompt: connect to the drive

from google.colab import drive
drive.mount('/content/drive')

# Install the Kaggle API client.
!pip install kaggle

# Upload your Kaggle API credentials (kaggle.json).
# You can download this file from your Kaggle account settings.
# from google.colab import files
# files.upload()

# # Move the Kaggle API key to the correct location.
# !mkdir -p ~/.kaggle
# !cp kaggle.json ~/.kaggle/
# !chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d raddar/chest-xrays-indiana-university

# Download the Roco dataset from Kaggle.
import os


# Specify the desired download path within Google Drive.
data_path = '/content/drive/MyDrive/ColabNotebooks/data'  # Replace with your desired path


print('Data source import complete.')

# List contents of the directory
print(os.listdir(data_path))

file_path = os.path.join(data_path, 'indiana_reports.csv')
print(file_path)
projections_path = '/content/drive/MyDrive/ColabNotebooks/indiana_projections.csv'
print(projections_path)
img_base_path = os.path.join(data_path, 'Hr_images/data/iu_hr_images/val')
print(img_base_path)
img_base_path_lr= os.path.join(data_path, 'Lr_Images/data/upscaled_imgs')
print(img_base_path_lr)

import pandas as pd
import numpy as np
import tqdm

from PIL import Image
import matplotlib.pyplot as plt

!pip install accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7

from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence

import torchvision
from torchvision import datasets, models, transforms
from torchvision.io import read_image, ImageReadMode
from torchvision.datasets import ImageFolder
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    HfArgumentParser,
    TrainingArguments,
    pipeline,
    logging,
)
from peft import LoraConfig, PeftModel
from trl import SFTTrainer

if torch.cuda.is_available():
    device = torch.device("cuda")
    print(f"Using GPU: {torch.cuda.get_device_name(0)}")
else:
    device = torch.device("cpu")
    print("Using CPU")

"""# Dataset visualization"""

img_base_dir = f'{img_base_path}/'
img_base_dir_lr = f'{img_base_path_lr}/'

reports_df = pd.read_csv(file_path)
reports_df.head()

projections_df = pd.read_csv(projections_path)
projections_df.head()

# @title Distribution of Projections

import matplotlib.pyplot as plt

# Assuming your data is in a pandas DataFrame called 'projections_df'
projection_counts = projections_df['projection'].value_counts()

plt.figure(figsize=(6, 6))
plt.pie(projection_counts, labels=projection_counts.index, autopct='%1.1f%%', startangle=90)
_ = plt.title('Distribution of Projections')

reports_df.shape, projections_df.shape

projections_df.uid.value_counts().describe()

class ImageCaptionData:
    def __init__(self, reports_df, projections_df):
        self.reports_df = reports_df.set_index('uid')
        self.projections_df = projections_df.set_index('uid')
        self.uids = reports_df[reports_df.findings.notnull()].uid.unique()

    def get_sample(self):
        uid = np.random.choice(self.uids)
        images = list(self.projections_df.loc[[uid]]['filename'])
        projections = list(self.projections_df.loc[[uid]]['projection'])
        findings = self.reports_df.loc[uid]['findings']
        return uid, images, projections, findings

paired_dataset = ImageCaptionData(reports_df, projections_df)
uid, images, projections, findings = paired_dataset.get_sample()

def display_sample(uid, images, projections, findings):
    plt.figure(figsize=(10, 5))
    print("UID:", uid)
    for i, (img, proj) in enumerate(zip(images, projections)):
        plt.subplot(1, len(images), i+1)
        png_img = Image.open(os.path.join(img_base_dir, img))
        png_img = png_img.convert('RGB')
        plt.title(proj)
        plt.imshow(png_img)
        plt.axis('off')
    plt.show()
    print("Findings:", findings)

display_sample(101, images, projections, findings)

uids = projections_df.uid.unique()
train_uids, test_uids = train_test_split(uids, test_size=0.1, random_state=42)

"""# Setting up Language Model"""

# The model that you want to train from the Hugging Face hub
model_name = "NousResearch/Llama-2-7b-chat-hf"

# The instruction dataset to use
dataset_name = "Shrey-1329/cxiu_hf_dataset"

# Fine-tuned model name
new_model = "Llama-2-7b-chat-finetune"

################################################################################
# QLoRA parameters
################################################################################

# LoRA attention dimension
lora_r = 64

# Alpha parameter for LoRA scaling
lora_alpha = 16

# Dropout probability for LoRA layers
lora_dropout = 0.1

################################################################################
# bitsandbytes parameters
################################################################################

# Activate 4-bit precision base model loading
use_4bit = True

# Compute dtype for 4-bit base models
bnb_4bit_compute_dtype = "float16"

# Quantization type (fp4 or nf4)
bnb_4bit_quant_type = "nf4"

# Activate nested quantization for 4-bit base models (double quantization)
use_nested_quant = False

################################################################################
# TrainingArguments parameters
################################################################################

# Output directory where the model predictions and checkpoints will be stored
output_dir = "./results"

# Number of training epochs
num_train_epochs = 1

# Enable fp16/bf16 training (set bf16 to True with an A100)
fp16 = False
bf16 = False

# Batch size per GPU for training
per_device_train_batch_size = 4

# Batch size per GPU for evaluation
per_device_eval_batch_size = 4

# Number of update steps to accumulate the gradients for
gradient_accumulation_steps = 1

# Enable gradient checkpointing
gradient_checkpointing = True

# Maximum gradient normal (gradient clipping)
max_grad_norm = 0.3

# Initial learning rate (AdamW optimizer)
learning_rate = 2e-4

# Weight decay to apply to all layers except bias/LayerNorm weights
weight_decay = 0.001

# Optimizer to use
optim = "paged_adamw_32bit"

# Learning rate schedule
lr_scheduler_type = "cosine"

# Number of training steps (overrides num_train_epochs)
max_steps = -1

# Ratio of steps for a linear warmup (from 0 to learning rate)
warmup_ratio = 0.03

# Group sequences into batches with same length
# Saves memory and speeds up training considerably
group_by_length = True

# Save checkpoint every X updates steps
save_steps = 0

# Log every X updates steps
logging_steps = 25

################################################################################
# SFT parameters
################################################################################

# Maximum sequence length to use
max_seq_length = None

# Pack multiple short examples in the same input sequence to increase efficiency
packing = False

# Load the entire model on the GPU 0
device_map = {"": 0}

# Load dataset (you can process it here)
dataset = load_dataset(dataset_name, split="train")

# Load tokenizer and model with QLoRA configuration
compute_dtype = getattr(torch, bnb_4bit_compute_dtype)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=use_4bit,
    bnb_4bit_quant_type=bnb_4bit_quant_type,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=use_nested_quant,
)

# Check GPU compatibility with bfloat16
if compute_dtype == torch.float16 and use_4bit:
    major, _ = torch.cuda.get_device_capability()
    if major >= 8:
        print("=" * 80)
        print("Your GPU supports bfloat16: accelerate training with bf16=True")
        print("=" * 80)

# Load base model
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map=device_map
)
model.config.use_cache = False
model.config.pretraining_tp = 1

# Load LLaMA tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right" # Fix weird overflow issue with fp16 training

# Load LoRA configuration
peft_config = LoraConfig(
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_r,
    bias="none",
    task_type="CAUSAL_LM",
)

# Set training parameters
training_arguments = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    optim=optim,
    save_steps=save_steps,
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    weight_decay=weight_decay,
    fp16=fp16,
    bf16=bf16,
    max_grad_norm=max_grad_norm,
    max_steps=max_steps,
    warmup_ratio=warmup_ratio,
    group_by_length=group_by_length,
    lr_scheduler_type=lr_scheduler_type,
    report_to="tensorboard"
)

# Set supervised fine-tuning parameters
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    tokenizer=tokenizer,
    args=training_arguments,
    packing=packing,
)

# Train model
# trainer.train()

sample_reports = ['Minimal XXXX opacities at the posterior sulci. A few septal lines of the left lateral sulcus. Otherwise, The lungs are clear with granulomas and XXXX sulci. Heart size upper normal thin LV contour.Unfolded calcified aorta. T-spine small osteophytes.',
 'The XXXX examination consists of frontal and lateral radiographs of the chest. The cardiomediastinal contours are within normal limits. Pulmonary vascularity is within normal limits. No focal consolidation, pleural effusion, or pneumothorax identified. Multilevel degenerative changes are seen throughout the thoracic spine. XXXX anchors XXXX over the left humeral head. There is mild bilateral acromioclavicular joint osteoarthritis. Visualized upper abdomen is grossly unremarkable in appearance.',
 'Lungs are hyperexpanded. Bullae are present in the upper lobes. No focal infiltrates. Heart size normal.',
 'Cardiomediastinal silhouette are normal in size and contour. Again demonstrated are biapical bullous emphysematous changes. No focal consolidation, pneumothorax, or pleural effusion. Mild multilevel degenerative changes of the thoracic spine.',
 'Cardiac and mediastinal contours are within normal limits. The lungs are clear. Bony structures are intact.']

model.eval()
generated_texts = []
with torch.no_grad():
    for smp_report in sample_reports:
        prompt_text = " ".join(smp_report.split()[:5])
        encoded_input = tokenizer.encode(prompt_text, return_tensors='pt')
        encoded_input = encoded_input.to(device)
        generated_sequences = model.generate(
            input_ids=encoded_input,
            max_length=100,  # Total length of output text (including the prompt)
            num_return_sequences=1,  # Number of sequences to generate
            temperature=0.7,  # Sampling temperature
            top_k=50,  # Top-k filtering
            top_p=0.95,  # Top-p (nucleus) filtering
            no_repeat_ngram_size=2,  # Prevent repetitions of n-grams
            pad_token_id=tokenizer.eos_token_id
        )
        generated_text = tokenizer.decode(generated_sequences[0], skip_special_tokens=True)
        generated_texts.append(generated_text)
for smp, gen in zip(sample_reports, generated_texts):
    print("Sample report:\n", smp)
    print("Generated report:\n", gen)
    print("------------")

trainer.train()

from huggingface_hub import login
from huggingface_hub import PyTorchModelHubMixin
login()

sample_reports[0]

model.eval()
generated_texts = []
with torch.no_grad():
    for smp_report in sample_reports:
        prompt_text = " ".join(smp_report.split()[:5])
        encoded_input = tokenizer.encode(prompt_text, return_tensors='pt')
        encoded_input = encoded_input.to(device)
        generated_sequences = model.generate(
            input_ids=encoded_input,
            max_length=100,  # Total length of output text (including the prompt)
            num_return_sequences=1,  # Number of sequences to generate
            temperature=0.7,  # Sampling temperature
            top_k=50,  # Top-k filtering
            top_p=0.95,  # Top-p (nucleus) filtering
            no_repeat_ngram_size=2,  # Prevent repetitions of n-grams
            pad_token_id=tokenizer.eos_token_id
        )
        generated_text = tokenizer.decode(generated_sequences[0], skip_special_tokens=True)
        generated_texts.append(generated_text)
for smp, gen in zip(sample_reports, generated_texts):
    print("Sample report:\n", smp)
    print("Generated report:\n", gen)
    print("------------")

new_model_path = "/content/drive/MyDrive/ColabNotebooks/Llama-2-7b-chat-finetune"

tokenizer = AutoTokenizer.from_pretrained(new_model_path)

vision_preprocess = models.swin_transformer.Swin_T_Weights.IMAGENET1K_V1.transforms()

MAX_SEQ_LENGTH  = 128
MAX_IMG_STACK = 5+1
BATCH_SIZE = 4

img_dir = '/content/drive/MyDrive/chest-xrays-indiana-university/images/images_normalized'

# prompt: print an image
# img_dir = '/content/drive/MyDrive/chest-xrays-indiana-university/images/images_normalized'
image_path = '/content/drive/MyDrive/chest-xrays-indiana-university/images/images_normalized/3399_IM-1643-2001.dcm.png'

import matplotlib.pyplot as plt
import os
from PIL import Image

def display_image(image_path):
    try:
        img = Image.open(image_path)
        img = img.convert('RGB') # Ensure image is in RGB format
        plt.imshow(img)
        plt.axis('off') # Hide axes
        plt.show()
    except FileNotFoundError:
        print(f"Error: Image file not found at {image_path}")
    except Exception as e:
        print(f"An error occurred: {e}")

# Example usage (assuming you have image_paths defined)
# Replace with your actual image path
# for image_path in image_paths:
#     display_image(image_path)

# Example with a single hardcoded path:
# image_path = '/content/drive/MyDrive/chest-xrays-indiana-university/images/images_normalized/00000001_000.png'
display_image(image_path)

class ImageCaptionData(Dataset):
    def __init__(self, reports_df, projections_df, max_length=MAX_SEQ_LENGTH, max_img_stack=MAX_IMG_STACK):
        self.reports_df = reports_df.dropna(subset={'findings'}).set_index('uid')
        self.projections_df = projections_df.set_index('uid')
        self.uids = list(set(self.reports_df.index).intersection(self.projections_df.index))
        self.max_length = max_length
        self.max_img_stack = max_img_stack

    def __getitem__(self, index):
        uid = self.uids[index]
        image_paths = [os.path.join(img_dir, x) for x in list(self.projections_df.loc[[uid]]['filename'])]
        images = torch.stack([vision_preprocess(read_image(img ,ImageReadMode.RGB)) for img in image_paths])
        len_image_stack = len(image_paths)
        img_dim = images[0].shape
        images = torch.cat((torch.zeros((self.max_img_stack - len_image_stack, *img_dim)), images), dim=0)

        findings = tokenizer.encode(self.reports_df.loc[uid]['findings'])
        findings = torch.tensor(findings[:self.max_length-1]+ [tokenizer.eos_token_id])
        return len_image_stack, images, findings

    def __len__(self):
        return len(self.uids)

def collate_batch(batch):
    """
    Pad the batch to ensure all sequences have the same length.
    """
    len_images = torch.tensor([item[0] for item in batch])
    img_data = torch.stack([item[1] for item in batch])
    pad_id = tokenizer.pad_token_id
    if pad_id is None:
        pad_id = tokenizer.eos_token_id

    tokens = pad_sequence([item[2] for item in batch], batch_first=True, padding_value=pad_id)
    labels = pad_sequence([torch.cat((torch.tensor([-100]*MAX_IMG_STACK), item[2])) for item in batch], batch_first=True, padding_value=pad_id)
    return [len_images, img_data, tokens, labels, pad_sequence([torch.ones(len(item[2])+MAX_IMG_STACK) for item in batch], batch_first=True, padding_value=0)]

train_dataset = ImageCaptionData(reports_df[reports_df.uid.isin(train_uids)], projections_df[projections_df.uid.isin(train_uids)])
test_dataset = ImageCaptionData(reports_df[~reports_df.uid.isin(train_uids)], projections_df[~projections_df.uid.isin(train_uids)])

train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)
test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)

x = next(iter(train_dataloader))
x[0].shape, x[1].shape, x[2].shape, x[3].shape, x[4].shape

# from transformers import LlamaForCausalLM, LlamaTokenizer
from peft import PeftModel, PeftConfig
from transformers import AutoModelForCausalLM

#config = PeftConfig.from_pretrained("Rithvik762/Llama-2-7b-chat-finetune")
#langmodel = PeftModel.from_pretrained(model, "Rithvik762/Llama-2-7b-chat-finetune")

# Vocabulary size
vocab_size = tokenizer.vocab_size
print(f"Vocabulary size: {vocab_size}")

# Model's maximum sequence length
max_length = tokenizer.model_max_length
print(f"Maximum sequence length: {max_length}")

# Hidden size (embedding dimension)
hidden_size = langmodel.config.hidden_size
print(f"Input embedding size: {hidden_size}")

vision_model = models.swin_t(weights = models.swin_transformer.Swin_T_Weights.DEFAULT)
vision_model.head = nn.Identity() # strip off classification layer of swin transformer to get the image embedding

VISION_MODEL_OUTPUT_DIM = 768
LANG_MODEL_INPUT_DIM = 4096

class ProjectionModel(nn.Module):
    def __init__(self, vision_out_dim, lang_inp_dim=4096):  # Set default lang_inp_dim to 4096
        super(ProjectionModel, self).__init__()
        self.lin = nn.Linear(vision_out_dim, lang_inp_dim, bias=True)

    def forward(self, x):
        x = nn.functional.tanh(self.lin(x))
        return x

# projection_model = ProjectionModel(VISION_MODEL_OUTPUT_DIM, LANG_MODEL_INPUT_DIM)
projection_model = ProjectionModel(vision_out_dim=768, lang_inp_dim=4096).to(device)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

vision_model = vision_model.to(device)
lang_model = langmodel.to(device)
projection_model = projection_model.to(device)

# freeze vision model
vision_model.eval()
for param in vision_model.parameters():
    param.requires_grad = False

optimizer = torch.optim.Adam(
    [
        {"params": lang_model.parameters(), "lr": 2e-5},
        {"params": projection_model.parameters(), "lr": 5e-5}
    ]
)

NUM_EPOCHS = 5

MODEL_CHECKPOINTS_PATH = '/content/drive/MyDrive/ColabNotebooks/checkpoints'
if not os.path.exists(MODEL_CHECKPOINTS_PATH):
    os.makedirs(MODEL_CHECKPOINTS_PATH)

import pdb

best_val_loss = np.inf

train_loss = []
for i in range(NUM_EPOCHS):
    train_batch_loss = []

    lang_model.train()
    projection_model.train()
    for l_img, img, tokens, labels, attn in tqdm.auto.tqdm(train_dataloader):
        img, tokens, labels, attn = img.to(device), tokens.to(device), labels.to(device), attn.to(device)
        optimizer.zero_grad()
        with torch.no_grad():
            # refetching bos_embedding to handle cases where bos_embedding might be trainable as well
            bos_embedding = lang_model.get_input_embeddings()(torch.tensor([tokenizer.bos_token_id]*MAX_IMG_STACK).to(device))
            bos_embedding = torch.stack([bos_embedding]*len(l_img))
            mask = torch.stack([torch.cat([torch.ones(MAX_IMG_STACK-limg), torch.zeros(limg)]).repeat(LANG_MODEL_INPUT_DIM,1).T for limg in l_img]).to(device)
            img_embed = vision_model(img.flatten(0,1))

        img_embed = projection_model(img_embed).reshape(len(l_img), MAX_IMG_STACK, LANG_MODEL_INPUT_DIM)
        img_embed = bos_embedding*mask + (1-mask)*img_embed # replace images that are not present with bos embedding

        tok_embed = lang_model.get_input_embeddings()(tokens)
        input_embed = torch.cat((img_embed, tok_embed), dim=1)
        outputs = lang_model(
                inputs_embeds=input_embed,
                labels=labels,
                attention_mask=attn,
            )

        loss = outputs.loss
        loss.backward()
        optimizer.step()
        train_batch_loss.append(loss.item()/len(l_img))

    train_loss.append(train_batch_loss)

    lang_model.eval()
    projection_model.eval()
    aggregated_val_loss = []
    with torch.no_grad():
        for l_img, img, tokens, labels, attn in tqdm.auto.tqdm(test_dataloader):
            img, tokens, labels, attn = img.to(device), tokens.to(device), labels.to(device), attn.to(device)

            bos_embedding = lang_model.get_input_embeddings()(torch.tensor([tokenizer.bos_token_id]*MAX_IMG_STACK).to(device))
            bos_embedding = torch.stack([bos_embedding]*len(l_img))
            mask = torch.stack([torch.cat([torch.ones(MAX_IMG_STACK-limg), torch.zeros(limg)]).repeat(LANG_MODEL_INPUT_DIM,1).T for limg in l_img]).to(device)
            img_embed = vision_model(img.flatten(0,1))

            img_embed = projection_model(img_embed).reshape(len(l_img), MAX_IMG_STACK, LANG_MODEL_INPUT_DIM)
            img_embed = bos_embedding*mask + (1-mask)*img_embed

            tok_embed = lang_model.get_input_embeddings()(tokens)
            input_embed = torch.cat((img_embed, tok_embed), dim=1)
            outputs = lang_model(
                    inputs_embeds=input_embed,
                    labels=labels,
                    attention_mask=attn,
                )

            loss = outputs.loss
            aggregated_val_loss.append(loss.item()/len(l_img))

    print("Epoch:", i, "Train Loss: {:.4f}".format(np.mean(train_batch_loss)), "Val Loss: {:.4f}".format(np.mean(aggregated_val_loss)))
    if (np.mean(aggregated_val_loss)<best_val_loss):
        best_val_loss = np.mean(aggregated_val_loss)
        epoch_checkpoint_path = os.path.join(MODEL_CHECKPOINTS_PATH, 'epoch_'+str(i))
        if not os.path.exists(epoch_checkpoint_path):
            os.makedirs(epoch_checkpoint_path)
        tokenizer.save_pretrained(epoch_checkpoint_path)
        trainer.model.save_pretrained(epoch_checkpoint_path)
        print("Saved checkpoint at "+ epoch_checkpoint_path)

BEST_CHECKPOINT_PATH = "/content/drive/MyDrive/ColabNotebooks/checkpoints/epoch_2"

VISION_MODEL_OUTPUT_DIM = 768
LANG_MODEL_INPUT_DIM = 4096

class ProjectionModel(nn.Module):
    def __init__(self, vision_out_dim, lang_inp_dim):
        super(ProjectionModel, self).__init__()
        self.lin = nn.Linear(vision_out_dim, lang_inp_dim, bias=True)

    def forward(self, x):
        x = nn.functional.tanh(self.lin(x))
        return x

def load_best_checkpoint(checkpoint_path):
    vision_preprocess = models.swin_transformer.Swin_T_Weights.IMAGENET1K_V1.transforms()

    config = PeftConfig.from_pretrained(checkpoint_path)
    lang_model = PeftModel.from_pretrained(model,checkpoint_path)
    projection_model = ProjectionModel(VISION_MODEL_OUTPUT_DIM, LANG_MODEL_INPUT_DIM)
    projection_model.load_state_dict(torch.load(os.path.join(checkpoint_path, 'projection_model.pth')))

    vision_model = models.swin_t(weights = models.swin_transformer.Swin_T_Weights.DEFAULT)
    vision_model.head = nn.Identity()

    vision_model.eval()
    projection_model.eval()
    lang_model.eval()
    return vision_model, projection_model, lang_model, tokenizer, vision_preprocess

vision_model, projection_model, lang_model, tokenizer, vision_preprocess = load_best_checkpoint(BEST_CHECKPOINT_PATH)

vision_model = vision_model.to(device)
lang_model = lang_model.to(device)
projection_model = projection_model.to(device)

# prompt: print the tokenizer.eos_token_id

print(tokenizer.bos_token_id)

def generate_report(image_paths):
    images = torch.stack([vision_preprocess(read_image(img,ImageReadMode.RGB)) for img in image_paths])
    len_image_stack = len(image_paths)
    with torch.no_grad():
        img_embed = vision_model(images.to(device))
        img_embed = projection_model(img_embed)

        # I found that padding here with bos token was actually causing issue with generation
        # probably because for gpt-2 bos_token=eos_token
        padded_img_embed = torch.cat([lang_model.get_input_embeddings()(torch.tensor([tokenizer.bos_token_id]*(MAX_IMG_STACK-len_image_stack)).to(device)), img_embed])

        generate_config = {
                    "eos_token_id": tokenizer.eos_token_id,
                    "bos_token_id": tokenizer.bos_token_id,
                    "pad_token_id": tokenizer.bos_token_id,
                    "max_new_tokens": 100,
                }

        output_ids = lang_model.generate(
            inputs_embeds=padded_img_embed.unsqueeze(0), **generate_config
        )
    return tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]



from google.colab import drive
import os
import pandas as pd

drive.mount('/content/drive')

# Paths to the CSV files and image directory
projections_csv_path = '/content/drive/MyDrive/ColabNotebooks/indiana_projections.csv'
image_dir = '/content/drive/MyDrive/ColabNotebooks/data/upscaled_imgs'
output_csv_path = '/content/drive/MyDrive/ColabNotebooks/data/filtered_projections.csv'

# Load the projections CSV
projections_df = pd.read_csv(projections_csv_path)

# Get a list of filenames in the image directory
image_filenames = set(os.listdir(image_dir))

# Filter the dataframe to include only rows where the second column value ('filename')
# exists as a file in the image directory
filtered_df = projections_df[projections_df['filename'].isin(image_filenames)]

# Save the filtered dataframe to a new CSV file
filtered_df.to_csv(output_csv_path, index=False)

print(f"Filtered CSV saved to: {output_csv_path}")

from nltk.translate.bleu_score import sentence_bleu
from PIL import Image
import os
import matplotlib.pyplot as plt

def compare_report(uid):
    print("Patient UID:", uid)

    # Load low-res images
    low_res_images_df = projections_df[projections_df.uid == uid]
    low_res_image_paths = [os.path.join(img_base_dir_lr, img_path) for img_path in low_res_images_df.filename]
    print(low_res_image_paths)
    # Load high-res images
    high_res_images_df = projections_df[projections_df.uid == uid]
    high_res_image_paths = [os.path.join(img_base_dir, img_path) for img_path in high_res_images_df.filename]

    # Fetch the reference findings
    findings = reports_df[reports_df.uid == uid].findings.iloc[0]

    # Display low-resolution images with projections
    plt.figure(figsize=(10, 5))
    for i, (img, proj) in enumerate(zip(low_res_image_paths, list(low_res_images_df.projection))):
        if len(low_res_image_paths) > 1:
            plt.subplot(1, len(low_res_image_paths), i + 1)
        png_img = Image.open(img)
        png_img = png_img.convert('RGB')
        plt.title(f"Low Res - {proj}")
        plt.imshow(png_img)
        plt.axis('off')
    plt.show()

    # Display high-resolution images with projections
    plt.figure(figsize=(10, 5))
    for i, (img, proj) in enumerate(zip(high_res_image_paths, list(high_res_images_df.projection))):
        if len(high_res_image_paths) > 1:
            plt.subplot(1, len(high_res_image_paths), i + 1)
        png_img = Image.open(img)
        png_img = png_img.convert('RGB')
        plt.title(f"High Res - {proj}")
        plt.imshow(png_img)
        plt.axis('off')
    plt.show()

    print("Findings:", findings)

    # Generate captions for both sets of images (low and high resolution)
    low_res_generated_report = generate_report(low_res_image_paths)
    high_res_generated_report = generate_report(high_res_image_paths)

    print("Generated Report (Low Resolution):", low_res_generated_report)
    print("Generated Report (High Resolution):", high_res_generated_report)

    # Tokenize findings and generated reports
    reference_tokens = [findings.split()]  # Assuming findings is a single string, split into words
    low_res_generated_tokens = low_res_generated_report.split()
    high_res_generated_tokens = high_res_generated_report.split()

    # Calculate BLEU score for both low and high resolution
    low_res_bleu_score = sentence_bleu(reference_tokens, low_res_generated_tokens, weights=(0.25, 0.25, 0.25, 0.25))
    high_res_bleu_score = sentence_bleu(reference_tokens, high_res_generated_tokens, weights=(0.25, 0.25, 0.25, 0.25))

compare_report(np.random.choice(test_uids))

compare_report(2070)

compare_report(np.random.choice(test_uids))

uid = np.random.choice(test_uids)
compare_report(uid)

uid = np.random.choice(test_uids)
compare_report(uid)